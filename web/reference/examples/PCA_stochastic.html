<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of PCA_stochastic</title>
  <meta name="keywords" content="PCA_stochastic">
  <meta name="description" content="Example of stochastic gradient algorithm in Manopt on a PCA problem.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../index.html">Home</a> &gt;  <a href="index.html">examples</a> &gt; PCA_stochastic.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../index.html"><img alt="<" border="0" src="../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for examples&nbsp;<img alt=">" border="0" src="../right.png"></a></td></tr></table>-->

<h1>PCA_stochastic
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="box"><strong>Example of stochastic gradient algorithm in Manopt on a PCA problem.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="box"><strong>function [X, A] = PCA_stochastic(A, k) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Example of stochastic gradient algorithm in Manopt on a PCA problem.
 
 PCA (principal component analysis) on a dataset A of size nxd consists
 in solving
 
   minimize_X  f(X) = -.5*norm(A*X, 'fro')^2 / n,
 
 where X is a matrix of dimension dxk with orthonormal columns. This
 is equivalent to finding k dominant singular vectors of A, or k top
 eigenvectors of A'*A.
 
 If n is large, this computation can be expensive. Thus,  stochastic
 gradient algorithms take the point of view that f(X) is a sum of many (n)
 terms: each term involves only one of the n rows of A.

 To make progress, it may be sufficient to optimize with respect to a
 subset of the terms at each iteration. This way, each individual
 iteration can be very cheap. In particular, individual operations have
 cost independent of n, because f or its gradient need never be evaluated
 completely (or at all in the case of f.)

 Stochastic gradient algorithms (this implementation in particular) are
 sensitive to proper parameter tuning. See in code.</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../matlabicon.gif)">
<li><a href="../manopt/manifolds/stiefel/stiefelfactory.html" class="code" title="function M = stiefelfactory(n, p, k, gpuflag)">stiefelfactory</a>	Returns a manifold structure to optimize over orthonormal matrices.</li><li><a href="../manopt/solvers/stochasticgradient/stochasticgradient.html" class="code" title="function [x, info, options] = stochasticgradient(problem, x, options)">stochasticgradient</a>	Stochastic gradient (SG) minimization algorithm for Manopt.</li><li><a href="../manopt/tools/statsfunhelper.html" class="code" title="function statsfun = statsfunhelper(inp1, inp2)">statsfunhelper</a>	Helper tool to create a statsfun for the options structure of solvers.</li></ul>
This function is called by:
<ul style="list-style-image:url(../matlabicon.gif)">
</ul>
<!-- crossreference -->

<h2><a name="_subfunctions"></a>SUBFUNCTIONS <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<ul style="list-style-image:url(../matlabicon.gif)">
<li><a href="#_sub1" class="code">function G = partialegrad(X, sample)</a></li></ul>

<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [X, A] = PCA_stochastic(A, k)</a>
0002 <span class="comment">% Example of stochastic gradient algorithm in Manopt on a PCA problem.</span>
0003 <span class="comment">%</span>
0004 <span class="comment">% PCA (principal component analysis) on a dataset A of size nxd consists</span>
0005 <span class="comment">% in solving</span>
0006 <span class="comment">%</span>
0007 <span class="comment">%   minimize_X  f(X) = -.5*norm(A*X, 'fro')^2 / n,</span>
0008 <span class="comment">%</span>
0009 <span class="comment">% where X is a matrix of dimension dxk with orthonormal columns. This</span>
0010 <span class="comment">% is equivalent to finding k dominant singular vectors of A, or k top</span>
0011 <span class="comment">% eigenvectors of A'*A.</span>
0012 <span class="comment">%</span>
0013 <span class="comment">% If n is large, this computation can be expensive. Thus,  stochastic</span>
0014 <span class="comment">% gradient algorithms take the point of view that f(X) is a sum of many (n)</span>
0015 <span class="comment">% terms: each term involves only one of the n rows of A.</span>
0016 <span class="comment">%</span>
0017 <span class="comment">% To make progress, it may be sufficient to optimize with respect to a</span>
0018 <span class="comment">% subset of the terms at each iteration. This way, each individual</span>
0019 <span class="comment">% iteration can be very cheap. In particular, individual operations have</span>
0020 <span class="comment">% cost independent of n, because f or its gradient need never be evaluated</span>
0021 <span class="comment">% completely (or at all in the case of f.)</span>
0022 <span class="comment">%</span>
0023 <span class="comment">% Stochastic gradient algorithms (this implementation in particular) are</span>
0024 <span class="comment">% sensitive to proper parameter tuning. See in code.</span>
0025 
0026 <span class="comment">% This file is part of Manopt and is copyrighted. See the license file.</span>
0027 <span class="comment">%</span>
0028 <span class="comment">% Main author: Bamdev Mishra and Nicolas Boumal, Sept. 6, 2017</span>
0029 <span class="comment">% Contributors:</span>
0030 <span class="comment">%</span>
0031 <span class="comment">% Change log:</span>
0032 <span class="comment">%</span>
0033 
0034 
0035     <span class="comment">% If none is given, generate a random data set: n samples in R^d</span>
0036     <span class="keyword">if</span> ~exist(<span class="string">'A'</span>, <span class="string">'var'</span>) || isempty(A)
0037         d = 1000;
0038         n = 100000;
0039         fprintf(<span class="string">'Generating data...'</span>);
0040         A = randn(n, d)*diag([[15 10 5], ones(1, d-3)]);
0041         fprintf(<span class="string">' done (size: %d x %d).\n'</span>, size(A));
0042     <span class="keyword">else</span>
0043         [n, d] = size(A);
0044     <span class="keyword">end</span>
0045 
0046     <span class="comment">% Pick a number of component to compute</span>
0047     <span class="keyword">if</span> ~exist(<span class="string">'k'</span>, <span class="string">'var'</span>) || isempty(k)
0048         k = 3;
0049     <span class="keyword">end</span>
0050     
0051     <span class="comment">% We are looking for k orthonormal vectors in R^d: Stiefel manifold.</span>
0052     problem.M = <a href="../manopt/manifolds/stiefel/stiefelfactory.html" class="code" title="function M = stiefelfactory(n, p, k, gpuflag)">stiefelfactory</a>(d, k);
0053     
0054     <span class="comment">% The cost function to minimize is a sum of n terms. This parameter</span>
0055     <span class="comment">% must be set for stochastic algorithms.</span>
0056     problem.ncostterms = n;
0057     
0058     <span class="comment">% We do not need to specify how to compute the value of the cost</span>
0059     <span class="comment">% function (stochastic algorithms never use this). All we need is to</span>
0060     <span class="comment">% specify how to compute the gradient of the cost function, where the</span>
0061     <span class="comment">% sum is restricted to a subset of the terms (a sample). Notice that we</span>
0062     <span class="comment">% specify a partial Euclidean gradient (hence the 'e' in partialegrad).</span>
0063     <span class="comment">% This way, Manopt will automatically convert the Euclidean vector into</span>
0064     <span class="comment">% a proper Riemannian partial gradient, in the tangent space at X.</span>
0065     <span class="comment">% In particular, if sample = 1:n, then the partial gradient corresponds</span>
0066     <span class="comment">% to the actual (complete) gradient.</span>
0067     problem.partialegrad = @<a href="#_sub1" class="code" title="subfunction G = partialegrad(X, sample)">partialegrad</a>;
0068     <a name="_sub1" href="#_subfunctions" class="code">function G = partialegrad(X, sample)</a>
0069         
0070         <span class="comment">% X is an orthonormal matrix of size dxk</span>
0071         <span class="comment">% sample is a vector if indices between 1 and n: a subset</span>
0072         <span class="comment">% Extract a subset of the dataset</span>
0073         Asample = A(sample, :);
0074         
0075         <span class="comment">% Compute the gradient of f restricted to that sample</span>
0076         G = -Asample'*(Asample*X);
0077         G = G / n;
0078         
0079     <span class="keyword">end</span>
0080 
0081     <span class="comment">% If one wants to use checkgradient to verify one's work, then it is</span>
0082     <span class="comment">% necessary to specify the cost function as well, as below.</span>
0083     <span class="comment">% problem.cost = @(X) -.5*norm(A*X, 'fro')^2 / n;</span>
0084     <span class="comment">% checkgradient(problem); pause;</span>
0085 
0086     <span class="comment">% To have the solver record statistics every x iterations, set</span>
0087     <span class="comment">% options.checkperiod to x. This will record simple quantities which</span>
0088     <span class="comment">% are almost free to compute (namely, elapsed time and step size of the</span>
0089     <span class="comment">% last step.) To record more sophisticated quantities, you can use</span>
0090     <span class="comment">% options.statsfun as usual. Time spent computing these statistics is</span>
0091     <span class="comment">% not counted in times reported in the info structure returned by the</span>
0092     <span class="comment">% solver.</span>
0093     options.checkperiod = 10;
0094     options.statsfun = <a href="../manopt/tools/statsfunhelper.html" class="code" title="function statsfun = statsfunhelper(inp1, inp2)">statsfunhelper</a>(<span class="string">'metric'</span>, @(X) norm(A*X, <span class="string">'fro'</span>));
0095     
0096     <span class="comment">% Set the parameters for the solver: stochastic gradient algorithms</span>
0097     <span class="comment">% tend to be quite sensitive to proper tuning, especially regarding</span>
0098     <span class="comment">% step size selection. See the solver's documentation for details.</span>
0099     options.maxiter = 200;
0100     options.batchsize = 10;
0101     <span class="comment">% options.stepsize_type = 'decay';</span>
0102     options.stepsize_init = 1e2;
0103     options.stepsize_lambda = 1e-3;
0104     options.verbosity = 2;
0105     
0106     <span class="comment">% Run the solver</span>
0107     [X, info] = <a href="../manopt/solvers/stochasticgradient/stochasticgradient.html" class="code" title="function [x, info, options] = stochasticgradient(problem, x, options)">stochasticgradient</a>(problem, [], options);
0108     
0109     
0110     <span class="comment">% Plot the special metric recorded by options.statsfun</span>
0111     plot([info.iter], [info.metric], <span class="string">'.-'</span>);
0112     xlabel(<span class="string">'Iteration #'</span>);
0113     ylabel(<span class="string">'Frobenius norm of A*X'</span>);
0114     title(<span class="string">'Convergence of stochasticgradient on stiefelfactory for PCA'</span>);
0115     
0116     <span class="comment">% Add to that plot a reference: the globally optimal value attained if</span>
0117     <span class="comment">% the true dominant singular vectors are computed.</span>
0118     fprintf(<span class="string">'Running svds... '</span>);
0119     t = tic();
0120     [V, ~] = svds(A', k);
0121     fprintf(<span class="string">'done: %g [s] (note: svd may be faster)\n'</span>, toc(t));
0122     hold all;
0123     bound = norm(A*V, <span class="string">'fro'</span>);
0124     plot([info.iter], bound*ones(size([info.iter])), <span class="string">'--'</span>);
0125     hold off;
0126     
0127     legend(<span class="string">'Algorithm'</span>, <span class="string">'SVD bound'</span>, <span class="string">'Location'</span>, <span class="string">'SouthEast'</span>);
0128     
0129 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Mon 10-Sep-2018 11:48:06 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>